{
  "p": [
    "MAPREDUCE-2998",
    "Failing to contact Am/History for jobs: java.io.EOFException in DataInputStream"
  ],
  "(1) Log information": {
    "(1.1) Roles in this case": {
      "p": [
        "AM: server side JobClient: client side"
      ]
    },
    "(1.2) Symptoms": {
      "p": [
        "“I am getting an exception frequently when running my jobs on a single-node cluster. It happens with basically any job I run: sometimes the job will work, but most of the time I get this exception (in this case, I was running a simple wordcount from the examples jar - where I got the exception 4 times in a row, and then the job worked the fifth time I submitted it).”",
        "11/09/12 17:17:50 INFO mapred.YARNRunner: AppMaster capability = memory: 2048,",
        "11/09/12 17:17:51 INFO mapred.YARNRunner: Command to launch container for ApplicationMaster is : $JAVA_HOME/bin/java -Dhadoop.root.logger=DEBUG,console -Xmx1536m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1315847180566 6 <FAILCOUNT> 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr",
        "11/09/12 17:17:51 INFO mapred.ResourceMgrDelegate: Submitted application application_1315847180566_6 to ResourceManager",
        "11/09/12 17:17:51 INFO mapred.ClientCache: Connecting to HistoryServer at: 0.0.0.0:10020",
        "11/09/12 17:17:51 INFO ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC",
        "11/09/12 17:17:51 INFO mapred.ClientCache: Connected to HistoryServer at: 0.0.0.0:10020",
        "11/09/12 17:17:51 INFO ipc.HadoopYarnRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.mapreduce.v2.api.MRClientProtocol",
        "11/09/12 17:17:51 INFO mapreduce.Job: Running job: job_1315847180566_0006",
        "11/09/12 17:17:52 INFO mapreduce.Job: map 0% reduce 0%",
        "11/09/12 17:18:00 INFO mapred.ClientServiceDelegate: Tracking Url of JOB is <IP-ADDRESS>:55361",
        "11/09/12 17:18:00 INFO mapred.ClientServiceDelegate: Connecting to <IP-ADDRESS>:43465",
        "11/09/12 17:18:00 INFO ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC",
        "11/09/12 17:18:00 INFO ipc.HadoopYarnRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.mapreduce.v2.api.MRClientProtocolMRClientProtocol is used for the communication between client and AM",
        "11/09/12 17:18:01 INFO mapred.ClientServiceDelegate:Failed to contact AM/History for jobjob_1315847180566_0006 Will retry..",
        "java.lang.reflect.UndeclaredThrowableException",
        "at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:179)",
        "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
        "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
        "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
        "at java.lang.reflect.Method.invoke(Method.java:597)",
        "at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:237)",
        "at org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents(ClientServiceDelegate.java:276)",
        "at org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents(YARNRunner.java:547)",
        "at org.apache.hadoop.mapreduce.Job.getTaskCompletionEvents(Job.java:540)",
        "at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1144)",
        "at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1092)",
        "at org.apache.hadoop.examples.WordCount.main(WordCount.java:84)",
        "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
        "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
        "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
        "at java.lang.reflect.Method.invoke(Method.java:597)",
        "at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)",
        "at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)",
        "at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)",
        "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
        "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
        "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
        "at java.lang.reflect.Method.invoke(Method.java:597)",
        "at org.apache.hadoop.util.RunJar.main(RunJar.java:189)",
        "(1)Caused by: com.google.protobuf.ServiceException: java.io.IOException: Call to /<IP-ADDRESS>:43465 failed on local exception: java.io.EOFException",
        "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)",
        "at $Proxy8.getTaskAttemptCompletionEvents(Unknown Source)",
        "at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:172)",
        "... 23 more",
        "(2)Caused by: java.io.IOException: Call to /<IP-ADDRESS>:43465 failed on local exception: java.io.EOFException",
        "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1119)",
        "at org.apache.hadoop.ipc.Client.call(Client.java:1087)",
        "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)",
        "... 25 more",
        "(3)Caused by: java.io.EOFException",
        "at java.io.DataInputStream.readInt(DataInputStream.java:375)",
        "at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:816)",
        "at org.apache.hadoop.ipc.Client$Connection.run(Client.java:754)",
        "11/09/12 17:18:01 INFO mapreduce.Job: Job job_1315847180566_0006 failed with state FAILED",
        "11/09/12 17:18:01 INFO mapreduce.Job: Counters: 0"
      ]
    }
  },
  "(2) How to figure out the root cause based on logs": {
    "p": [
      "According to the logs and call-stack, we know that a WordCount job has been submitted, and JobClient connected to AM to getTaskCompletionEvents but failed.",
      "",
      "“This happens because AM exited when some call from client was in progress. This is expected in some cases. The retry should eventually figure out the final state of the job from RM and subsequently form JobHistoryServer. ”",
      "In the comments:",
      "• Looking at the NodeManager logs, I am seeing this:",
      "11/09/15 12:46:51 WARN monitor.ContainersMonitorImpl: Container [pid=22125,containerID=container_1316090748961_0001_01_000001] is running beyond memory-limits.Current usage: 2150408192bytes.Limit: 2147483648bytes.Killing container.",
      "Dump of the process-tree for container_1316090748961_0001_01_000001 :",
      "22125 21539 22125 22125 (bash) 1 1 65400832 291 /bin/bash -c /home/<USERNAME>/hadoop-build/jdk1.6.0_12/bin/java -Dhadoop.root.logger=DEBUG,console -Xmx1536m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1316090748961 1 1 1>/home/hadoop/mapred/nm/logs/application_1316090748961_0001/container_1316090748961_0001_01_000001/stdout 2>/home/",
      "",
      "From this NM log, which printed the information about AM, we see that for some reason the AM is using more memory now, which has exceeded the memery limits.",
      "The progress of task launch is as follows:",
      "Each map/reduce task may have multiple TaskAttempts(TA). In AM, when RMContainerAllocator gets the allocated container resource from RM, it will assign the container(s) to TA, and send TaskAttemptEventType.TA_ASSIGNEDevent. This event is handled by functiontransition()in classContainerAssignedTransition, and TA’s state changes from unsigned to assigned:",
      "// Transitions from the UNASSIGNED state. (in TaskAttemptImpl.java)",
      ".addTransition(TaskAttemptState.UNASSIGNED,",
      "TaskAttemptState.ASSIGNED, TaskAttemptEventType.TA_ASSIGNED,",
      "newContainerAssignedTransition())",
      "InContainerAssignedTransition.transition(), the object ContainerLaunchContext(CLC) will be created:",
      "//create the container object to be launched for a given Task attempt",
      "ContainerLaunchContext launchContext =createContainerLaunchContext(",
      "cEvent.getApplicationACLs(), taskAttempt.containerID,",
      "taskAttempt.conf, taskAttempt.jobToken, taskAttempt.remoteTask,",
      "taskAttempt.oldJobId, taskAttempt.assignedCapability,",
      "taskAttempt.jvmID, taskAttempt.taskAttemptListener,",
      "taskAttempt.credentials);",
      "ContainerLaunchContext(CLC) object describes resources needed by the NM to lauch a container: local resources, environment variables, command to execute etc.",
      "When creating CLC, function getInitialClasspath() is called:",
      "Apps.addToEnvironment(environment, Environment.CLASSPATH.name(),getInitialClasspath());",
      "",
      "The definition ofgetInitialClasspath():",
      "/*Lock this on initialClasspath so that there is only one fork in the AM for",
      "* getting the initial class-path.TODO: This should go away once we construct",
      "* a parent CLC and use it for all the containers. */",
      "privateStringgetInitialClasspath()throwsIOException {",
      "synchronized(classpathLock) {",
      "if(initialClasspathFlag.get()) {returninitialClasspath; }",
      "Map<String, String> env =newHashMap<String, String>();",
      "MRApps.setClasspath(env);",
      "initialClasspath= env.get(Environment.CLASSPATH.name());",
      "initialClasspathFlag.set(true);",
      "returninitialClasspath;",
      "}",
      "}",
      "",
      "According to the comments of this function. First time when this function is called, it will fork a sub-process to launch \"bin/mapred classpath” command to figure out the classpath(where to get the Hadoop jar and the required libraries).",
      "",
      "Before the fix, the object “classpathLock” in keyword synchronized is “final” type, so when AM starts several tasks, there is possibility that several sub-processes will be forked to get classpath, and each sub-process takes an equal amount of Vmem.The more the number of tasks, the more chance of this happening. As a result, the process which is running AM will exceed its memory limit."
    ]
  },
  "(3) Root Cause": {
    "p": [
      "Multiple forks for lauching “bin/mapred” in AM lead to AM’s abnormal memory consumption, and finally make AM exit. So the jobClient failed to contact AM."
    ]
  },
  "(4) Fixing Method": {
    "p": [
      "This file is running in AM.",
      "• hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
      "@@ -153,7 +153,7 @@ public abstract class TaskAttemptImpl implements",
      "private Token<JobTokenIdentifier> jobToken;",
      "private static AtomicBoolean initialClasspathFlag = new AtomicBoolean();",
      "private static String initialClasspath = null;",
      "-private final Object classpathLock = new Object();",
      "+privatestaticfinal Object classpathLock = new Object();",
      "",
      "The object classpathLock is only used in getInitialClasspath() of this file.",
      "Change the classpathLock from object-level to class-level, to make sure there is only one fork in AM for getting classpath.",
      "/**",
      "* Lock this on initialClasspath so that there is only one fork in the AM for",
      "* getting the initial class-path. TODO: This should go away once we construct",
      "* a parent CLC and use it for all the containers.",
      "*/",
      "privateString getInitialClasspath()throwsIOException {",
      "synchronized(classpathLock) {",
      "if(initialClasspathFlag.get()) {returninitialClasspath; }",
      "Map<String, String> env =newHashMap<String, String>();",
      "MRApps.setClasspath(env);",
      "initialClasspath= env.get(Environment.CLASSPATH.name());",
      "initialClasspathFlag.set(true);",
      "returninitialClasspath;",
      "}",
      "}"
    ]
  },
  "(5) How many nodes are involved in the patch? (multiple/single node(s))": {
    "p": [
      "“TaskAttemptImpl.java” is running in AM, so the patch will act on each AM (one AM per Job)."
    ]
  },
  "(6) Code Snippets": {
    "p": [
      "According to the logs and call-stack, we know that a WordCount job has been submitted, and JobClient hopes to connect to AM to getTaskCompletionEvents.",
      "// Submit the job, then poll for progress until the job is complete",
      "job.waitForCompletion(true);",
      "",
      "public booleanwaitForCompletion(booleanverbosejob.java",
      ")throwsIOException, InterruptedException, ClassNotFoundException {",
      "if(state== JobState.DEFINE) { submit(); }",
      "if(verbose) {monitorAndPrintJob();}else{…}",
      "returnisSuccessful();",
      "}",
      "",
      "public booleanmonitorAndPrintJob()throwsIOException, InterruptedException {job.java",
      "String lastReport =null;",
      "Job.TaskStatusFilter filter;",
      "Configuration clientConf = getConfiguration();",
      "filter = Job.getTaskOutputFilter(clientConf);",
      "JobID jobId = getJobID();",
      "LOG.info(\"Running job: \"+ jobId);",
      "…",
      "intprogMonitorPollIntervalMillis =",
      "Job.getProgressPollInterval(clientConf);",
      "/* make sure to report full progress after the job is done */",
      "booleanreportedAfterCompletion =false;",
      "while(!isComplete() || !reportedAfterCompletion) {",
      "if(isComplete()) { reportedAfterCompletion =true;",
      "}else{ Thread.sleep(progMonitorPollIntervalMillis); }",
      "String report = (\" map \"+ StringUtils.formatPercent(mapProgress(),0)+",
      "\" reduce \"+ StringUtils.formatPercent(reduceProgress(),0));",
      "if(!report.equals(lastReport)) {LOG.info(report); lastReport = report; }",
      "TaskCompletionEvent[] events =getTaskCompletionEvents(eventCounter,10);",
      "eventCounter += events.length;",
      "printTaskEvents(events, filter, profiling, mapRanges, reduceRanges);",
      "}",
      "booleansuccess = isSuccessful();",
      "if(success) {…}",
      "Counters counters = getCounters();",
      "if(counters !=null) {LOG.info(counters.toString()); }",
      "returnsuccess;",
      "}",
      "publicTaskCompletionEvent[]getTaskCompletionEvents(intstartFrom,job.java",
      "intnumEvents)throwsIOException, InterruptedException {",
      "ensureState(JobState.RUNNING);",
      "returncluster.getClient().getTaskCompletionEvents(getJobID(), startFrom, numEvents);",
      "}",
      "",
      "publicTaskCompletionEvent[]getTaskCompletionEvents(JobID arg0,intarg1,intarg2)YarnRunner.java",
      "throwsIOException, InterruptedException {",
      "returnclientCache.getClient(arg0).getTaskCompletionEvents(arg0, arg1, arg2);",
      "}",
      "ClientServiceDelegate.java",
      "publicTaskCompletionEvent[]getTaskCompletionEvents(JobID arg0,intarg1,intarg2)",
      "throwsIOException, InterruptedException {",
      "org.apache.hadoop.mapreduce.v2.api.records.JobId jobID = TypeConverter.toYarn(arg0);",
      "GetTaskAttemptCompletionEventsRequest request =recordFactory",
      ".newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);",
      "request.setJobId(jobID);",
      "request.setFromEventId(arg1);",
      "request.setMaxEvents(arg2);",
      "List<org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent> list =",
      "((GetTaskAttemptCompletionEventsResponse)invoke(",
      "\"getTaskAttemptCompletionEvents\", GetTaskAttemptCompletionEventsRequest.class, request)).",
      "getCompletionEventList();",
      "returnTypeConverter",
      ".fromYarn(list.toArray(neworg.apache.hadoop.mapreduce.v2.api.records.TaskAttemptCompletionEvent[0]));",
      "}",
      "",
      "private synchronizedObjectinvoke(String method, Class argClass,ClientServiceDelegate.java",
      "Object args)throwsYarnRemoteException {",
      "Method methodOb =null;",
      "try{ methodOb = MRClientProtocol.class.getMethod(method, argClass);",
      "}catch(SecurityException e) {throw newYarnException(e);",
      "}catch(NoSuchMethodException e) {throw newYarnException(\"Method name mismatch\", e); }",
      "while(true) {",
      "try{returnmethodOb.invoke(getProxy(), args); }catch(YarnRemoteException yre) {",
      "LOG.warn(\"Exception thrown by remote end.\", yre);",
      "throw yre;",
      "}catch(InvocationTargetException e) { if (e.getTargetException() instanceof YarnRemoteException) {…}",
      "LOG.info(\"Failed to contact AM/History for job \"+jobId+\" retrying..\");",
      "LOG.debug(\"Failed exception on AM/History contact\", e.getTargetException());",
      "// Force reconnection by setting the proxy to null.",
      "realProxy = null;",
      "}catch(Exception e) {LOG.info(\"Failed to contact AM/History for job \" + jobId + \" Will retry..\");",
      "LOG.debug(\"Failing to contact application master\", e);",
      "// Force reconnection by setting the proxy to null.",
      "realProxy=null;",
      "}",
      "}",
      "}",
      "MRClientProtocolPBClientImpl.java",
      "publicGetTaskAttemptCompletionEventsResponsegetTaskAttemptCompletionEvents(",
      "GetTaskAttemptCompletionEventsRequest request)throwsYarnRemoteException {",
      "GetTaskAttemptCompletionEventsRequestProto requestProto = ((GetTaskAttemptCompletionEventsRequestPBImpl)request).getProto();",
      "try{return newGetTaskAttemptCompletionEventsResponsePBImpl(proxy.getTaskAttemptCompletionEvents(null, requestProto));",
      "}catch(ServiceException e) {if(e.getCause()instanceofYarnRemoteException) {",
      "throw(YarnRemoteException)e.getCause();",
      "}else if(e.getCause()instanceofUndeclaredThrowableException) {",
      "throw(UndeclaredThrowableException)e.getCause();",
      "}else{throw newUndeclaredThrowableException(e); }",
      "}",
      "}",
      "",
      "Then based on (1)caused byProtoOverHadoopRpcEngine.java",
      "publicObjectinvoke(Object proxy, Method method, Object[] args)",
      "throwsThrowable {",
      "longstartTime =0;",
      "if(LOG.isDebugEnabled()) { startTime = System.currentTimeMillis(); }",
      "ProtoSpecificRpcRequest rpcRequest = constructRpcRequest(method, args);",
      "ProtoSpecificResponseWritable val =null;",
      "try{ val = (ProtoSpecificResponseWritable)client.call(",
      "newProtoSpecificRequestWritable(rpcRequest),remoteId);",
      "}catch(Exceptione) {throw newServiceException(e); }",
      "ProtoSpecificRpcResponse response = val.message;",
      "…",
      "}",
      "…",
      "}",
      "",
      "Based on (2)caused by,Client.java",
      "publicWritablecall(Writable param, ConnectionId remoteId)throwsInterruptedException, IOException {",
      "Call call =newCall(param);",
      "Connection connection = getConnection(remoteId, call);",
      "connection.sendParam(call);// send the parameter",
      "booleaninterrupted =false;",
      "synchronized(call) {",
      "while(!call.done) {try{ call.wait();// wait for the result",
      "}catch(InterruptedException ie) {// save the fact that we were interrupted",
      "interrupted =true; }",
      "}",
      "if(interrupted) {// set the interrupt flag now that we are done waiting",
      "Thread.currentThread().interrupt();",
      "}",
      "if(call.error!=null) {",
      "if(call.errorinstanceofRemoteException) { call.error.fillInStackTrace();throwcall.error;",
      "}else{// local exception",
      "InetSocketAddress address = remoteId.getAddress();",
      "throwNetUtils.wrapException(address.getHostName(), address.getPort(),NetUtils.getHostname(),",
      "0,call.error); }",
      "}else{returncall.value; }",
      "}",
      "}",
      "Client.java",
      "publicstaticIOExceptionwrapException(finalString destHost,final intdestPort,finalString localHost,",
      "final intlocalPort,finalIOException exception) {",
      "if(exceptioninstanceofBindException) {return newBindException(…);",
      "}else if(exceptioninstanceofConnectException) {",
      "// connection refused; include the host:port in the error",
      "return(ConnectException)newConnectException(…);",
      "}else if(exceptioninstanceofUnknownHostException) {",
      "return(UnknownHostException)newUnknownHostException(…);",
      "}else if(exceptioninstanceofSocketTimeoutException) {",
      "return(SocketTimeoutException)newSocketTimeoutException(…);",
      "}else if(exceptioninstanceofNoRouteToHostException) {",
      "return(NoRouteToHostException)newNoRouteToHostException(…);",
      "}",
      "else{return(IOException)newIOException(\"Failed on local exception: “+ exception",
      "+ \"; Host Details : \"+ getHostDetailsAsString(destHost, destPort, localHost)).initCause(exception);",
      "}",
      "}",
      "",
      "Based on (3)caused by,",
      "at java.io.DataInputStream.readInt(DataInputStream.java:375)",
      "at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:816)",
      "at org.apache.hadoop.ipc.Client$Connection.run(Client.java:754)",
      "connection.run() {",
      "if(LOG.isDebugEnabled())",
      "LOG.debug(getName() + \": starting, having connections \" + connections.size());",
      "try{while(waitForWork()) { //wait here for work - read or close connection",
      "receiveResponse();",
      "}",
      "}catch(Throwable t) {",
      "// This truly is unexpected, since we catch IOException in receiveResponse",
      "// -- this is only to be really sure that we don't leave a client hanging",
      "// forever.",
      "LOG.warn(\"Unexpected error reading responses on connection \" + this, t);",
      "markClosed(new IOException(\"Error reading responses\", t));",
      "}",
      "close();",
      "if(LOG.isDebugEnabled())LOG.debug(getName() + \": stopped, remaining connections \" + connections.size());",
      "}",
      "",
      "/* Receive a response.Because only one receiver, so no synchronization on in. */",
      "private voidreceiveResponse() {",
      "if(shouldCloseConnection.get()) {return; }",
      "touch();",
      "try{intid =in.readInt(); // try to read an id",
      "if(LOG.isDebugEnabled())LOG.debug(getName() + \" got value #\"+ id);",
      "Call call =calls.get(id);",
      "intstate =in.readInt(); // read call status",
      "if(state == Status.SUCCESS.state) { Writable value = ReflectionUtils.newInstance(valueClass,conf);",
      "value.readFields(in); // read value",
      "call.setValue(value);",
      "calls.remove(id);",
      "}else if(state == Status.ERROR.state) {",
      "call.setException(newRemoteException(WritableUtils.readString(in), WritableUtils.readString(in)));",
      "calls.remove(id);",
      "}else if(state == Status.FATAL.state) { // Close the connection",
      "markClosed(newRemoteException(WritableUtils.readString(in), WritableUtils.readString(in)));",
      "}",
      "}catch(IOExceptione) {markClosed(e); }",
      "}",
      "",
      "EOFException : if this input stream reaches the end before reading four bytes.",
      "public final intreadInt()throwsIOException{",
      "intch1 =in.read();",
      "intch2 =in.read();",
      "intch3 =in.read();",
      "intch4 =in.read();",
      "if((ch1 | ch2 | ch3 | ch4) <0)throw newEOFException();",
      "return((ch1 <<24) + (ch2 <<16) + (ch3 <<8) + (ch4 <<0));",
      "}",
      "private synchronized voidmarkClosed(IOException e) {",
      "if(shouldCloseConnection.compareAndSet(false,true)) {closeException= e;",
      "notifyAll();",
      "}",
      "}"
    ]
  }
}