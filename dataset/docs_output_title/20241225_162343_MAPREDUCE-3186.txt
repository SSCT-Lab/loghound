{
  "p": [
    "MAPREDUCE-3186",
    "User jobs are getting hanged if the Resource manager process goes down and comes up while job is getting executed."
  ],
  "(1) Log information": {
    "If the resource manager is restarted while the job execution is in progress, the job is getting hanged.": {},
    "UI shows the job as running.": {},
    "In the RM log, it is throwing an error": {
      "p": [
        "ERROR org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AppAttemptId doesnt exist in cache appattempt_1318579738195_0004_000001"
      ]
    }
  },
  "(2) How to figure out the root cause based on logs": {
    "p": [
      "The ERROR from RM is printed in ApplicationMasterService.java",
      "publicAllocateResponseallocate(AllocateRequest request)throwsYarnRemoteException {",
      "...",
      "/* check if its in cache */",
      "AllocateResponse allocateResponse =recordFactory",
      ".newRecordInstance(AllocateResponse.class);",
      "AMResponse lastResponse =responseMap.get(appAttemptId);",
      "if(lastResponse ==null) {",
      "LOG.error(\"AppAttemptId doesnt exist in cache \"+ appAttemptId);",
      "allocateResponse.setAMResponse(reboot);",
      "returnallocateResponse;",
      "}",
      "...",
      "}",
      "After ERROR is printed, a “reboot” command will be sent to the AM.",
      "However, the AM is not reboot as expected, instead, the job is hanging.",
      "Then the developers check the behavior of current AM with the corresponding source code.",
      "",
      "AM sends heartbeat to RM through the container allocator thread, which is implemented in RMCommunicator.java:",
      "protected voidstartAllocatorThread() {",
      "allocatorThread=newThread(newRunnable() {",
      "@Override",
      "public voidrun() {",
      "while(!stopped&& !Thread.currentThread().isInterrupted()) {",
      "try{",
      "Thread.sleep(rmPollInterval);",
      "try{",
      "heartbeat();",
      "}catch(Exception e) {",
      "LOG.error(\"ERROR IN CONTACTING RM. \", e);",
      "}",
      "}catch(InterruptedException e) {",
      "LOG.info(\"Allocated thread interrupted. Returning.\");",
      "return;",
      "}",
      "}",
      "}",
      "});",
      "allocatorThread.setName(\"RMCommunicator Allocator\");",
      "allocatorThread.start();",
      "}",
      "",
      "heartbeat() is implemented in RMContainerAllocator.java",
      "protected synchronized voidheartbeat()throwsException {",
      "LOG.info(\"Before Scheduling: \"+ getStat());",
      "List<Container> allocatedContainers =getResources();",
      "...",
      "}",
      "",
      "getResources() will send heartbeat (may contain container request) to RM and receive the response.",
      "privateList<Container>getResources()throwsException {",
      "intheadRoom = getAvailableResources() !=null? getAvailableResources().getMemory() :0;//first time it would be null",
      "AMResponseresponse=makeRemoteRequest();",
      "intnewHeadRoom = getAvailableResources() !=null? getAvailableResources().getMemory() :0;",
      "List<Container> newContainers = response.getAllocatedContainers();",
      "List<ContainerStatus> finishedContainers = response.getCompletedContainersStatuses();",
      "...",
      "List<Container> allocatedContainers =newArrayList<Container>();",
      "for(Container cont : newContainers) {",
      "allocatedContainers.add(cont);",
      "LOG.debug(\"Received new Container :\"+ cont);",
      "}",
      "for(ContainerStatus cont : finishedContainers) {",
      "...",
      "}",
      "returnnewContainers;",
      "}",
      "",
      "But currently, the command returned by the reponse is not checked."
    ]
  },
  "(3) Root Cause": {
    "p": [
      "When the RM is stopped and restarted, the RM asks the MRAppMaster to “reboot”. But AM “ignores” this command and keeps contacting the restarted RM.",
      ""
    ]
  },
  "(4) Fixing Method": {
    "p": [
      "Two problmes are fixed by the patch:",
      "When AM receives the response from RM, it should check the reboot flag.",
      "When AM fails to connect the RM, the retry should be limited based on the exception type.",
      "•hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMCommunicator.java",
      "@@ -233,6 +233,9 @@",
      "Thread.sleep(rmPollInterval);",
      "try {",
      "heartbeat();",
      "+ } catch (YarnException e) {",
      "+ LOG.error(\"Error communicating with RM: \" + e.getMessage() , e);",
      "+ return;",
      "} catch (Exception e) {",
      "LOG.error(\"ERROR IN CONTACTING RM. \", e);",
      "// TODO: for other exceptions",
      "·hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "private List<Container> getResources() throws Exception {",
      "int headRoom = getAvailableResources() != null ? getAvailableResources().getMemory() : 0;//first time it would be null",
      "- AMResponse response = makeRemoteRequest();",
      "+ AMResponse response;",
      "+ try {",
      "+ response = makeRemoteRequest();",
      "+ // Reset retry count if no exception occurred.",
      "+ retrycount = getConfig().getInt(MRJobConfig.MR_AM_TO_RM_RETRIES,",
      "+ MRJobConfig.DEFAULT_MR_AM_TO_RM_RETRIES);",
      "+ } catch (Exception e) {",
      "+ // This can happen when the connection to the RM has gone down. Keep",
      "+ // re-trying until the retrycount is exhausted.",
      "+ if (retrycount-- <= 0) {",
      "+ eventHandler.handle(new JobEvent(this.getJob().getID(),",
      "+ JobEventType.INTERNAL_ERROR));",
      "+ throw new YarnException(\"Could not contact RM after \" +",
      "+ getConfig().getInt(MRJobConfig.MR_AM_TO_RM_RETRIES,",
      "+ MRJobConfig.DEFAULT_MR_AM_TO_RM_RETRIES) +",
      "+ \" tries.\");",
      "+ }",
      "+ throw e;",
      "+ }",
      "+ if (response.getReboot()) {",
      "+ // This can happen if the RM has been restarted. If it is in that state,",
      "+ // this job must clean itself up.",
      "+ eventHandler.handle(new JobEvent(this.getJob().getID(),",
      "+ JobEventType.INTERNAL_ERROR));",
      "+ throw new YarnException(\"Resource Manager doesn't recognize AttemptId: \" +",
      "+ this.getContext().getApplicationID());",
      "+ }",
      "int newHeadRoom = getAvailableResources() != null ? getAvailableResources().getMemory() : 0;",
      "List<Container> newContainers = response.getAllocatedContainers();",
      "List<ContainerStatus> finishedContainers = response.getCompletedContainersStatuses();",
      ""
    ]
  },
  "(5) How many nodes are involved in the patch": {
    "p": [
      "AM"
    ]
  }
}