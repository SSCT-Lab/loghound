{
  "p": [
    "MAPREDUCE-5466",
    "Historyserver does not refresh the result of restarted jobs after RM restart"
  ],
  "(1) Log information": {
    "p": [
      "Restart RM when sort job is running and verify that the job passes successfully after RM restarts.",
      "Once the job finishes successfully, run job status command for sort job. It shows \"Job state =FAILED\".",
      "",
      "hadoop job -status job_1375923346354_0003",
      "13/08/08 01:24:13 INFO mapred.ClientServiceDelegate: Application state iscompleted. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server",
      "Job: job_1375923346354_0003",
      "Job File: hdfs://host1:port1/history/done/2013/08/08/000000/job_1375923346354_0003_conf.xml",
      "Job Tracking URL :http://historyserver:port2/jobhistory/job/job_1375923346354_0003",
      "Uber job : false",
      "Number of maps: 80",
      "Number of reduces: 1",
      "map() completion: 0.0",
      "reduce() completion: 0.0",
      "Job state: FAILED",
      "retired: false",
      "reason for failure: There are no failed tasks for the job. Job is failed due to some other reason and reason can be found in the logs.",
      "Counters not available. Job is retired."
    ]
  },
  "(2) How to figure out the root cause based on logs": {
    "p": [
      "From the log, we see that the status returned by Job history server is not consistent with the result returned by RM. The history server did not not update the state for the job which finished succesfully after RM restarts.",
      "",
      "The command obtains the incorrect result by calling getJobStatus().",
      "As shown in the following call relationship, “getJobReport” is invoked through thehistoryServerProxyreturned bygetProxy()",
      "",
      "publicJobStatusgetJobStatus(JobID oldJobID)throwsIOException {",
      "org.apache.hadoop.mapreduce.v2.api.records.JobId jobId = TypeConverter.toYarn(oldJobID);",
      "...",
      "JobReport report = ((GetJobReportResponse)invoke(\"getJobReport\",",
      "GetJobReportRequest.class, request)).getJobReport();",
      "JobStatus jobStatus =null;",
      "...",
      "returnjobStatus;",
      "}",
      "",
      "private synchronizedObjectinvoke(String method, Class argClass,Object args)throwsIOException {",
      "Method methodOb =null;",
      "...",
      "while(maxRetries >0) {",
      "try{",
      "returnmethodOb.invoke(getProxy(), args);",
      "}catch(InvocationTargetException e) {",
      "...",
      "}",
      "throwlastException;",
      "}",
      "",
      "privateMRClientProtocolgetProxy()throwsIOException { ClientServiceDelegate.java",
      "...",
      "try{",
      "application =rm.getApplicationReport(appId);",
      "}catch(YarnException e2) {",
      "throw newIOException(e2);",
      "}",
      "...",
      "//History server can serve a job only if application succeeded.",
      "if(application.getYarnApplicationState() == YarnApplicationState.FINISHED) {",
      "LOG.info(\"Application state is completed. FinalApplicationStatus=\"",
      "+ application.getFinalApplicationStatus().toString()",
      "+\". Redirecting to job history server\");",
      "realProxy= checkAndGetHSProxy(application, JobState.SUCCEEDED);",
      "}",
      "returnrealProxy;",
      "}",
      "",
      "privateMRClientProtocol checkAndGetHSProxy(",
      "ApplicationReport applicationReport, JobState state) {",
      "if(null==historyServerProxy) {",
      "LOG.warn(\"Job History Server is not configured.\");",
      "returngetNotRunningJob(applicationReport, state);",
      "}",
      "returnhistoryServerProxy;",
      "}",
      "",
      "Then on historyServer side, the function call relationship is as follows:",
      "",
      "However, based on the limited log, it is difficult to figure out the root cause.",
      "publicHistoryFileInfogetFileInfo(JobId jobId)throwsIOException { HistoryFileManager.java",
      "// FileInfo available in cache.",
      "HistoryFileInfo fileInfo =jobListCache.get(jobId);",
      "if(fileInfo !=null) {",
      "returnfileInfo;",
      "}",
      "// OK so scan the intermediate to be sure we did not lose it that way",
      "scanIntermediateDirectory();",
      "fileInfo =jobListCache.get(jobId);",
      "if(fileInfo !=null) {",
      "returnfileInfo;",
      "}",
      "// Intermediate directory does not contain job. Search through older ones.",
      "fileInfo = scanOldDirsForJob(jobId);",
      "if(fileInfo !=null) {",
      "returnfileInfo;",
      "}",
      "return null;",
      "}",
      "The state of each job is written into userDir by JobHistoryEventHandler."
    ]
  },
  "(3) Root Cause": {
    "p": [
      "When multiple threads call createPassword() almost simultaneously, they all try to visit the variable secretkeys, this may cause data race since the variable is not threadsafe."
    ]
  },
  "(4) Fixing Method": {
    "p": [
      "Fixing the error.",
      "Change secretkeys from HashMap to ConcurrentHashMap.",
      "The fixed file is on the stack trace.",
      "•hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/security/ContainerTokenSecretManager.java",
      "@@ -34,9 +34,9 @@ public class ContainerTokenSecretManager extends",
      "private static Log LOG = LogFactory",
      ".getLog(ContainerTokenSecretManager.class);",
      "- private Map<String, SecretKey> secretkeys =",
      "- new HashMap<String, SecretKey>();",
      "-",
      "+ Map<String, SecretKey>secretkeys=",
      "+ newConcurrentHashMap<String, SecretKey>();",
      "+",
      "// Used by master for generation of secretyKey per host",
      "public SecretKey createAndGetSecretKey(CharSequence hostName) {",
      "String hostNameStr = hostName.toString();",
      ""
    ]
  },
  "(5) How many nodes are involved in the patch": {
    "p": [
      "The patch will work on RM",
      ""
    ]
  }
}