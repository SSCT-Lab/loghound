{
  "p": [
    "HDFS-5080",
    "BootstrapStandby not working with QJM when the existing NN is active"
  ],
  "(1) Log information": {
    "(1.1) Roles in this case": {
      "p": [
        "StandbyNameNode (client-side) gets Txid from Active NameNode (server-side)",
        "StandbyNameNode (client-side) gets Txid from Quorum Journal Nodes(server-side)",
        "(server-side)"
      ]
    },
    "(1.2) Symptoms": {
      "p": [
        "The log is from StandByNamenode(SBN):",
        "FATALha.BootstrapStandby: Unable to read transaction ids 6175397-6175405 from the configured shared edits storage. Please copy these logs into the shared edits storage or call saveNamespace on the active node.",
        "Error: Gap in transactions. Expected to be able to read up until at least txid 6175405 but unable to find any edit logs containing txid 6175405",
        "java.io.IOException: Gap in transactions. Expected to be able to read up until at least txid 6175405 but unable to find any edit logs containing txid 6175405",
        "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.checkForGaps(FSEditLog.java:1300)",
        "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1258)",
        "at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.checkLogsAvailableForRead(BootstrapStandby.java:229)",
        "When Bootstrap StandbyNN, it checks whether there are gaps within the edit logs.",
        "And it shows that there is gap in edits/transactions: unable to find edit logs containing txid 6175405. (All the operations in HDFS are called edits or transactions.)"
      ]
    }
  },
  "(2) How to figure out the root cause based on logs": {
    "p": [
      "(2.1) The architecture in HA (High Availability) mode with QJM (Quorum Journal Manager):",
      "The implementation of the Quorum Journal protocol consists of two parts:",
      "·Quorum Journal Manager (QJM)that runs inside name node and takes some of the responsibilities of the leader in a consensus algorithm.",
      "·Journal Nodes (JNs)that actually replicate and persist the HDFS edit logs to file system.",
      "When any namespace modification is performed by the Active NN, it durably logs a record of the modification to a majority of these JNs. The Standby NN is capable of reading the edits from the JNs. As the Standby Node sees the edits, it applies them to its own namespace. In the event of a failover, the Standby will ensure that it has read all of the edits from the JounalNodes before promoting itself to the Active state.",
      "(2.2) When running the tool “BootstrapStandy”, StandyNN’s storage directories will be bootstrapped by copying the latest namespace snapshot from the active NN.",
      "Based on the call-stack and source code, the control flow is as follows:",
      "",
      "",
      "Function checkLogsAvailableForRead () is to make sure that we have enough edits already in the shared directory(i.e. JNs) to start up from the last checkpoint on the activeNN.",
      "The transaction ids obtained form Active NN are:",
      "imageTxId= proxy.getMostRecentCheckpointTxId();",
      "curTxId= proxy.getTransactionID();",
      "The transaction ids to be checked are form JNs (image).",
      "When the active NN is queried by BootstrapStandby about the last written transaction ID, the in-progress edit log segment (i.e., txid-6175405) is included.",
      "However, when journal nodes are asked about the last written transaction ID, in-progress edit log(i.e. txid-6175405) is not contained: ActiveNN hasn’t finished committing the edit (txid-6175405) to QJM, so StandbyNN cannot read it from JNs."
    ]
  },
  "(3) Root Cause": {
    "p": [
      "The txids obtained by StandbyNN from ActiveNN and JNs are not consistent, which causes BootstrapStandby#checkLogsAvailableForRead to complain gaps.",
      ""
    ]
  },
  "(4) Fixing Method": {
    "p": [
      "The idea of the patch is : control the txid obtained from Active NN by setting Boolean variables “forReading” and “inProgessOk”.",
      "inProgessOkset to true if in-progress streams are OK",
      "forReadingwhether or not to use the streams to load the edit log",
      "· If the corresponding request is not for loading/reading editlog (e.g., just get the transaction id), i.e. forReading = false, whether including the in-progress txid is decided by \"inProgressOk” value.",
      "· If the corresponding request is for loading/reading editlog (e.g, bootstrapStanby), i.e. forReading = true, “inProgressOk” will be forcibly set false, so the in-progress txid will not be included.",
      "",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLogger.java (QJM-side)",
      "@@ -109,7 +109,7 @@ AsyncLogger createLogger(Configuration conf, NamespaceInfo nsInfo,",
      "* Fetch the list of edit logs available on the remote node.",
      "*/",
      "public ListenableFuture<RemoteEditLogManifest>getEditLogManifest(",
      "-long fromTxnId, boolean forReading);",
      "+long fromTxnId, boolean forReading,boolean inProgressOk);",
      "",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLoggerSet.java",
      "@@ -262,14 +262,14 @@ void appendHtmlReport(StringBuilder sb) {",
      "return QuorumCall.create(calls);",
      "}",
      "-public QuorumCall<AsyncLogger, RemoteEditLogManifest>",
      "-getEditLogManifest(long fromTxnId, boolean forReading) {",
      "+public QuorumCall<AsyncLogger, RemoteEditLogManifest>getEditLogManifest(",
      "+long fromTxnId, boolean forReading,boolean inProgressOk) {",
      "Map<AsyncLogger,",
      "ListenableFuture<RemoteEditLogManifest>> calls",
      "= Maps.newHashMap();",
      "for (AsyncLogger logger : loggers) {",
      "ListenableFuture<RemoteEditLogManifest> future =",
      "-logger.getEditLogManifest(fromTxnId, forReading);",
      "+logger.getEditLogManifest(fromTxnId, forReading,inProgressOk);",
      "calls.put(logger, future);",
      "}",
      "return QuorumCall.create(calls);",
      "",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java",
      "@@ -519,12 +519,13 @@ public Void call() throws Exception {",
      "@Override",
      "public ListenableFuture<RemoteEditLogManifest>getEditLogManifest(",
      "-final long fromTxnId, final boolean forReading) {",
      "+final long fromTxnId, final boolean forReading,",
      "+final boolean inProgressOk) {",
      "return executor.submit(new Callable<RemoteEditLogManifest>() {",
      "@Override",
      "public RemoteEditLogManifest call() throws IOException {",
      "GetEditLogManifestResponseProto ret = getProxy().getEditLogManifest(",
      "-journalId, fromTxnId, forReading);",
      "+journalId, fromTxnId, forReading,inProgressOk);",
      "// Update the http port, since we need this to build URLs to any of the",
      "// returned logs.",
      "httpPort = ret.getHttpPort();",
      "",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java",
      "@@ -456,7 +456,7 @@ public void selectInputStreams(Collection<EditLogInputStream> streams,",
      "long fromTxnId, boolean inProgressOk, boolean forReading) throws IOException {",
      "QuorumCall<AsyncLogger, RemoteEditLogManifest> q =",
      "-loggers.getEditLogManifest(fromTxnId, forReading);",
      "+loggers.getEditLogManifest(fromTxnId, forReading,inProgressOk);",
      "Map<AsyncLogger, RemoteEditLogManifest> resps =",
      "loggers.waitForWriteQuorum(q, selectInputStreamsTimeoutMs,",
      "\"selectInputStreams\");",
      "@@ -480,8 +480,7 @@ public void selectInputStreams(Collection<EditLogInputStream> streams,",
      "allStreams.add(elis);",
      "}",
      "}",
      "-JournalSet.chainAndMakeRedundantStreams(",
      "-streams, allStreams, fromTxnId,inProgressOk);",
      "+JournalSet.chainAndMakeRedundantStreams(streams, allStreams, fromTxnId);",
      "}",
      "",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocol.java",
      "@@ -125,10 +125,13 @@ public void purgeLogsOlderThan(RequestInfo requestInfo, long minTxIdToKeep)",
      "* @param sinceTxId the first transaction which the client cares about",
      "* @param forReading whether or not the caller intends to read from the edit",
      "* logs",
      "+* @param inProgressOk whether or not to check the in-progress edit log",
      "+* segment",
      "* @return a list of edit log segments since the given transaction ID.",
      "*/",
      "-public GetEditLogManifestResponseProtogetEditLogManifest(",
      "-String jid, long sinceTxId, boolean forReading) throws IOException;",
      "+public GetEditLogManifestResponseProto getEditLogManifest(String jid,",
      "+long sinceTxId, boolean forReading,boolean inProgressOk)",
      "+throws IOException;",
      "",
      "",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolServerSideTranslatorPB.java",
      "@@ -203,7 +203,8 @@ public GetEditLogManifestResponseProtogetEditLogManifest(",
      "return impl.getEditLogManifest(",
      "request.getJid().getIdentifier(),",
      "request.getSinceTxId(),",
      "-request.getForReading());",
      "+request.getForReading(),",
      "+request.getInProgressOk());",
      "} catch (IOException e) {",
      "throw new ServiceException(e);",
      "}",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolTranslatorPB.java",
      "@@ -228,13 +228,15 @@ public void purgeLogsOlderThan(RequestInfo reqInfo, long minTxIdToKeep)",
      "@Override",
      "public GetEditLogManifestResponseProtogetEditLogManifest(String jid,",
      "-long sinceTxId, boolean forReading) throws IOException {",
      "+long sinceTxId, boolean forReading,boolean inProgressOk)",
      "+throws IOException {",
      "try {",
      "return rpcProxy.getEditLogManifest(NULL_CONTROLLER,",
      "GetEditLogManifestRequestProto.newBuilder()",
      ".setJid(convertJournalId(jid))",
      ".setSinceTxId(sinceTxId)",
      ".setForReading(forReading)",
      "+.setInProgressOk(inProgressOk)",
      ".build());",
      "} catch (ServiceException e) {",
      "throw ProtobufHelper.getRemoteException(e);",
      "",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java(QJN-side)",
      "@@ -630,14 +630,31 @@ private void purgePaxosDecision(long segmentTxId) throws IOException {",
      "* @see QJournalProtocol#getEditLogManifest(String, long)",
      "*/",
      "public RemoteEditLogManifestgetEditLogManifest(long sinceTxId,",
      "-boolean forReading) throws IOException {",
      "+boolean forReading, booleaninProgressOk) throws IOException {",
      "// No need to checkRequest() here - anyone may ask for the list",
      "// of segments.",
      "checkFormatted();",
      "",
      "-RemoteEditLogManifest manifest = new RemoteEditLogManifest(",
      "-fjm.getRemoteEditLogs(sinceTxId, forReading));",
      "-return manifest;",
      "+// if this is for reading, ignore the in-progress editlog segment",
      "+inProgressOk = forReading ? false : inProgressOk;",
      "+List<RemoteEditLog> logs = fjm.getRemoteEditLogs(sinceTxId, forReading,+inProgressOk);",
      "+",
      "+if (inProgressOk) {",
      "+RemoteEditLog log = null;",
      "+for (Iterator<RemoteEditLog> iter = logs.iterator(); iter.hasNext();) {",
      "+log = iter.next();",
      "+if (log.isInProgress()) {",
      "+iter.remove();",
      "+break;",
      "+}",
      "+}",
      "+if (log != null && log.isInProgress()) {",
      "+logs.add(new RemoteEditLog(log.getStartTxId(), getHighestWrittenTxId()));",
      "+}",
      "+}",
      "+",
      "+return new RemoteEditLogManifest(logs);",
      "}",
      "",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeRpcServer.java",
      "@@ -175,10 +175,11 @@ public void purgeLogsOlderThan(RequestInfo reqInfo, long minTxIdToKeep)",
      "@Override",
      "public GetEditLogManifestResponseProto getEditLogManifest(String jid,",
      "-long sinceTxId, boolean forReading) throws IOException {",
      "+long sinceTxId, boolean forReading, booleaninProgressOk)",
      "+throws IOException {",
      "",
      "RemoteEditLogManifest manifest = jn.getOrCreateJournal(jid)",
      "-.getEditLogManifest(sinceTxId, forReading);",
      "+.getEditLogManifest(sinceTxId, forReading,inProgressOk);",
      "",
      "return GetEditLogManifestResponseProto.newBuilder()",
      ".setManifest(PBHelper.convert(manifest))",
      "",
      "",
      "•hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
      "@@ -1274,6 +1274,7 @@ synchronized void recoverUnclosedStreams() {",
      "}",
      "}",
      "+ @Override",
      "public void selectInputStreams(Collection<EditLogInputStream> streams,",
      "long fromTxId, boolean inProgressOk, boolean forReading) {",
      "journalSet.selectInputStreams(streams, fromTxId, inProgressOk, forReading);",
      "@@ -1284,18 +1285,27 @@ public void selectInputStreams(Collection<EditLogInputStream> streams,",
      "return selectInputStreams(fromTxId, toAtLeastTxId, null, true);}",
      "+/** Select a list of input streams to load */",
      "+public Collection<EditLogInputStream>selectInputStreams(This method is on the exception call stack",
      "+long fromTxId, long toAtLeastTxId, MetaRecoveryContext recovery,",
      "+boolean inProgressOk) throws IOException {",
      "+return selectInputStreams(fromTxId, toAtLeastTxId, recovery, inProgressOk,",
      "+true);",
      "+}",
      "+",
      "/**",
      "-* Select a list of input streams to load.",
      "+* Select a list of input streams.",
      "*",
      "* @param fromTxId first transaction in the selected streams",
      "* @param toAtLeast the selected streams must contain this transaction",
      "* @param inProgessOk set to true if in-progress streams are OK",
      "+* @param forReading whether or not to use the streams to load the edit log",
      "*/",
      "public synchronized Collection<EditLogInputStream>selectInputStreams(",
      "long fromTxId, long toAtLeastTxId, MetaRecoveryContext recovery,",
      "-boolean inProgressOk) throws IOException {",
      "+boolean inProgressOk,booleanforReading) throws IOException {",
      "List<EditLogInputStream> streams = new ArrayList<EditLogInputStream>();",
      "-selectInputStreams(streams, fromTxId, inProgressOk, true);",
      "+selectInputStreams(streams, fromTxId, inProgressOk, forReading);",
      "try {",
      "checkForGaps(streams, fromTxId, toAtLeastTxId, inProgressOk);",
      "",
      "• hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
      "(Get editlog manifests from the active namenode for all the editlogs)",
      "@@ -169,18 +169,26 @@ public void purgeLogsOlderThan(long minTxIdToKeep)",
      "* @param fromTxId the txnid which to start looking",
      "* @param forReading whether or not the caller intends to read from the edit",
      "* logs",
      "+* @param inProgressOk whether or not to include the in-progress edit log",
      "+* segment",
      "* @return a list of remote edit logs",
      "* @throws IOException if edit logs cannot be listed.",
      "*/",
      "public List<RemoteEditLog>getRemoteEditLogs(long firstTxId,",
      "-boolean forReading) throws IOException {",
      "+boolean forReading, boolean inProgressOk) throws IOException {",
      "+// make sure not reading in-progress edit log, i.e., if forReading is true,",
      "+// we should ignore the in-progress edit log.",
      "+Preconditions.checkArgument(!(forReading && inProgressOk));",
      "+",
      "File currentDir = sd.getCurrentDir();",
      "List<EditLogFile> allLogFiles = matchEditLogs(currentDir);",
      "List<RemoteEditLog> ret = Lists.newArrayListWithCapacity(",
      "allLogFiles.size());",
      "for (EditLogFile elf : allLogFiles) {",
      "-if (elf.hasCorruptHeader() || elf.isInProgress()) continue;",
      "+if (elf.hasCorruptHeader() || (!inProgressOk && elf.isInProgress())) {",
      "+continue;",
      "+}",
      "if (elf.getFirstTxId() >= firstTxId) {",
      "ret.add(new RemoteEditLog(elf.firstTxId, elf.lastTxId));",
      "} else if (elf.getFirstTxId() < firstTxId && firstTxId <= elf.getLastTxId()) {",
      "",
      "• hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java",
      "@@ -257,13 +255,12 @@ public void selectInputStreams(Collection<EditLogInputStream> streams,",
      "\". Skipping.\", ioe);",
      "}",
      "}",
      "-chainAndMakeRedundantStreams(streams, allStreams, fromTxId,inProgressOk);",
      "+chainAndMakeRedundantStreams(streams, allStreams, fromTxId);",
      "}",
      "",
      "public static void chainAndMakeRedundantStreams(",
      "Collection<EditLogInputStream> outStreams,",
      "-PriorityQueue<EditLogInputStream> allStreams,",
      "-long fromTxId,boolean inProgressOk) {",
      "+PriorityQueue<EditLogInputStream> allStreams, long fromTxId) {",
      "// We want to group together all the streams that start on the same start",
      "// transaction ID. To do this, we maintain an accumulator (acc) of all",
      "// the streams we've seen at a given start transaction ID. When we see a",
      "@@ -598,7 +595,7 @@ public synchronized RemoteEditLogManifest getEditLogManifest(long fromTxId,",
      "if (j.getManager() instanceof FileJournalManager) {",
      "FileJournalManager fjm = (FileJournalManager)j.getManager();",
      "try {",
      "-allLogs.addAll(fjm.getRemoteEditLogs(fromTxId, forReading));",
      "+allLogs.addAll(fjm.getRemoteEditLogs(fromTxId, forReading, false));",
      "} catch (Throwable t) {",
      "LOG.warn(\"Cannot list edit logs in \" + fjm, t);",
      "}",
      "",
      "• hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.javaThis method is on the exception call stack@@ -226,7 +226,7 @@ private booleancheckLogsAvailableForRead(FSImage image, long imageTxId,",
      "try {",
      "Collection<EditLogInputStream> streams =",
      "image.getEditLog().selectInputStreams(",
      "-firstTxIdInLogs, curTxIdOnOtherNode, null, true);",
      "+firstTxIdInLogs, curTxIdOnOtherNode, null, true, false);",
      "for (EditLogInputStream stream : streams) {",
      "IOUtils.closeStream(stream);",
      "}",
      "",
      "• hadoop-hdfs-project/hadoop-hdfs/src/main/proto/QJournalProtocol.proto",
      "@@ -177,6 +177,7 @@ message GetEditLogManifestRequestProto {",
      "required uint64 sinceTxId = 2; // Transaction ID",
      "// Whether or not the client will be reading from the returned streams.",
      "optional bool forReading = 3 [default = true];",
      "+optional bool inProgressOk = 4 [default = false];",
      "}",
      "message GetEditLogManifestResponseProto {",
      ""
    ]
  },
  "(5) How many nodes are involved in the patch": {
    "p": [
      "The patch will be applied in Journal Nodes and NameNode."
    ]
  },
  "(6) Notes": {
    "p": [
      "ImageTxId: TxId of the last transaction that was included in the most recent fsimage file. This does not include any transactions that have since been written to the edit log.",
      "curTxId: max(lastAppliedTxId,getLastWrittenTxId())",
      "lastAppliedTxId: The last transaction ID that was either loaded from an image or loaded by loading edits files.",
      "getLastWrittenTxId(): the transaction ID of the last transaction written to the log.",
      "",
      "StandbyNN ➡ JNs ➡ ActiveNN",
      ""
    ]
  }
}