{
  "p": [
    "Cassandra-5105",
    "repair -pr throws EOFException"
  ],
  "(1)Log information": {
    "p": [
      "There are 3 nodes in the cluster and the problem occurred between 01(/10.80.90.52)and 02(/10.80.90.52), we can see that 02 throws an exception when receiving the stream, which causes a Broken Pipe exception on the 01 side.",
      "cassandra02 (/10.80.90.52)",
      "INFO 15:53:36,532 [streaming task #aa70f8e0-656c-11e2-b226-d966287ae7ca]Receivedtask from /10.80.90.51 to stream 7871 ranges to /10.80.90.53",
      "INFO 15:53:36,533 [streaming task #aa70f8e0-656c-11e2-b226-d966287ae7ca] Performing streaming repair of 7871 ranges with /10.80.90.53",
      "INFO 15:53:43,216 Stream context metadata [/var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4840-Data.db sections=5086 progress=0/350803667 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4787-Data.db sections=6079 progress=0/1160848303 - 0%, ..., /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4825-Data.db sections=5526 progress=0/571701570 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4879-Data.db sections=3043 progress=0/3086625 - 0%], 11 sstables.",
      "INFO 15:53:43,217 Streaming to /10.80.90.53",
      "INFO 15:53:43,325 Beginning transfer to /10.80.90.51",
      "INFO 15:53:43,362 Flushing memtables for [CFS(Keyspace='MyBusinessKeyspace', ColumnFamily='MyBusinessHistoryCF')]...",
      "INFO 15:53:43,363 Enqueuing flush of Memtable-MyBusinessHistoryCF@554695962(2424273/24900316 serialized/live bytes, 51289 ops)",
      "INFO 15:53:43,366 Writing Memtable-MyBusinessHistoryCF@554695962(2424273/24900316 serialized/live bytes, 51289 ops)",
      "INFO 15:53:43,558 Completed flushing /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4881-Data.db (2083586 bytes) for commitlog position ReplayPosition(segmentId=1358949372387, position=2780498)",
      "INFO 15:53:51,056 Stream context metadata [/var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4787-Data.db sections=6645 progress=0/1303785158 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4840-Data.db sections=5775 progress=0/392625457 - 0%, ..., /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4879-Data.db sections=4083 progress=0/3265613 - 0%], 12 sstables.",
      "INFO 15:53:51,057 Streaming to /10.80.90.51",
      "INFO 15:54:19,013 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4840-Data.db to /10.80.90.53",
      "ERROR15:54:46,686 Exception in thread Thread[Thread-3087,5,main]",
      "java.lang.RuntimeException:Last written key DecoratedKey(153906576608468125601485890282698016632, 72736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a496e7472616461793a66306135386333352d353262312d343361642d396430332d6130636630306330306565633a3937383a313330313137) >= current key DecoratedKey(33745288399064288388334698406389712581, bec0000000220a0b08f494e9b2b582a3301005121308......0b08f4bab8c7ddd8a330100500080000013c6679e1e10200093a805108d6a10004d3f04c1c040d0000000d0a0b08bedfbbd0000000450fe4ef100080000013c3d58dcf6010004d34fa32017330000000450fe4f3300080000013c3d5921a1010004d34fa42bd8c00000000450fe4f4500080000013c3d59abe1010004d34fa64b77240000000450fe4f6900080000013c3d5a29cd010004d34fa834f5ee0000000450fe4f8900080000013c3d5b24) writing into /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-tmp-ia-4882-Data.db",
      "at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)",
      "at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:209)",
      "at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)",
      "at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)",
      "at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:226)",
      "at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:166)",
      "at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:66)",
      "INFO 15:55:37,245 CFS(Keyspace='OpsCenter', ColumnFamily='pdps') liveRatio is 2.3633228082745292 (just-counted was 2.3633228082745292). calculation took 2ms for 1256 columns",
      "INFO 15:56:30,149 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4787-Data.db to /10.80.90.53",
      "INFO 15:56:32,401 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4878-Data.db to /10.80.90.53",
      "",
      "cassandra01(/10.80.90.51)",
      "INFO 15:53:43,296 Enqueuing flush of Memtable-MyBusinessHistoryCF@709374075(7410300/48461437 serialized/live bytes, 155356 ops)",
      "INFO 15:53:43,304 Writing Memtable-MyBusinessHistoryCF@709374075(7410300/48461437 serialized/live bytes, 155356 ops)",
      "INFO 15:53:43,688 Completed flushing /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4952-Data.db (4474554 bytes) for commitlog position ReplayPosition(segmentId=1358949134910, position=13864741)",
      "INFO 15:54:09,160 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4922-Data.db to /10.80.90.52",
      "INFO 15:54:11,605 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4922-Data.db to /10.80.90.53",
      "ERROR15:54:46,682 Exception in thread Thread[Streaming to /10.80.90.52:2,5,main]",
      "java.lang.RuntimeException: java.io.IOException: Broken pipe",
      "at com.google.common.base.Throwables.propagate(Throwables.java:160)",
      "at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)",
      "at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)",
      "at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)",
      "at java.lang.Thread.run(Unknown Source)",
      "Caused by: java.io.IOException: Broken pipe",
      "at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)",
      "at sun.nio.ch.FileChannelImpl.transferToDirectly(Unknown Source)",
      "at sun.nio.ch.FileChannelImpl.transferTo(Unknown Source)",
      "at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:90)",
      "at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)",
      "at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)",
      "... 3 more",
      ""
    ]
  },
  "(2) How to figure out the root cause based on logs": {
    "p": [
      "When the bug is reported, the reporter only provided the ERROR logs on both sides. Then one of the developers suggested that “Some logs before error happend may help if you can upload.” And from the log context, we see that the exception occurs on node-02 when it is receiving data from node-01, and later leading to the exception on node-01.",
      "private longbeforeAppend(DecoratedKey decoratedKey)",
      "{",
      "assertdecoratedKey !=null:\"Keys must not be null\";// empty keys ARE allowed b/c of indexed column values",
      "if(lastWrittenKey!=null&&lastWrittenKey.compareTo(decoratedKey) >=0)",
      "throw newRuntimeException(\"Last written key \"+lastWrittenKey+\" >= current key \"+ decoratedKey +\" writing into \"+ getFilename());",
      "return(lastWrittenKey==null) ?0:dataFile.getFilePointer();",
      "}",
      "The exception is thrown with in the above method, which performs check on the position to be written.",
      "And it seems that parameterdecoratedKeyis not valid.",
      "Based on the stack trace, we find the statement which passeddecoratedKeyto beforeAppend(). But it is hard to locate the error.",
      "Since the call stack on node-01 mentioned \"CompressedFileStreamTask\", the developers tried to disable the column family compaction and scrub it. Then the following repairs worked fine so they decided to re-enable the compression, which made the error come back.",
      "",
      "\"I found one problem that can send extra chunk to destination which causes reading from wrong position.",
      "This happens when the streaming section of sstable falls into the edge of compression chunks.”",
      "",
      "",
      "",
      "private staticList<PendingFile>createPendingFiles(Iterable<SSTableReader> sstables, Collection<Range<Token>> ranges, OperationType type)",
      "{",
      "List<PendingFile> pending =newArrayList<PendingFile>();",
      "for(SSTableReader sstable : sstables)",
      "{",
      "Descriptor desc = sstable.descriptor;",
      "List<Pair<Long,Long>> sections = sstable.getPositionsForRanges(ranges);",
      "if(sections.isEmpty())",
      "{",
      "// A reference was acquired on the sstable and we won't stream it",
      "sstable.releaseReference();",
      "continue;",
      "}",
      "CompressionInfo compression =null;",
      "if(sstable.compression)",
      "{",
      "compression =newCompressionInfo(sstable.getCompressionMetadata().getChunksForSections(sections),",
      "sstable.getCompressionMetadata().parameters);",
      "}",
      "pending.add(newPendingFile(sstable, desc, SSTable.COMPONENT_DATA, sections, type, sstable.estimatedKeysForRanges(ranges), compression));",
      "}",
      "logger.info(\"Stream context metadata {}, {} sstables.\", pending, Iterables.size(sstables));",
      "returnpending;",
      "}",
      "",
      "",
      "publicChunk[]getChunksForSections(Collection<Pair<Long, Long>> sections)",
      "{",
      "// use SortedSet to eliminate duplicates and sort by chunk offset",
      "SortedSet<Chunk> offsets =newTreeSet<Chunk>(newComparator<Chunk>()",
      "{",
      "...",
      "for(Pair<Long, Long> section : sections)",
      "{",
      "intstartIndex = (int) (section.left/parameters.chunkLength());",
      "intendIndex = (int) (section.right/parameters.chunkLength());",
      "+endIndex = section.right%parameters.chunkLength() ==0? endIndex -1: endIndex;",
      "for(inti = startIndex; i <= endIndex; i++)",
      "{",
      "longoffset = i *8;",
      "...",
      "}",
      "}",
      "returnoffsets.toArray(newChunk[offsets.size()]);",
      "}"
    ]
  },
  "(3) Root Cause": {
    "p": [
      "node-01 (sending side) sends extra chunk to destination (node-02, receiving side) which causes reading from wrong position, node-02 throws RuntimeException and the connection breaks (leading to broken pipe exception on node-1).",
      "This happens when the streaming section of sstable falls into the edge of compression chunks."
    ]
  },
  "(4) Fixing Method": {
    "p": [
      "Fixing the error. Modify the incorrect computation.",
      "•/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java",
      "publicChunk[]getChunksForSections(Collection<Pair<Long, Long>> sections)",
      "{",
      "// use SortedSet to eliminate duplicates and sort by chunk offset",
      "SortedSet<Chunk> offsets =newTreeSet<Chunk>(newComparator<Chunk>()",
      "{",
      "...",
      "for(Pair<Long, Long> section : sections)",
      "{",
      "intstartIndex = (int) (section.left/parameters.chunkLength());",
      "intendIndex = (int) (section.right/parameters.chunkLength());",
      "+endIndex = section.right%parameters.chunkLength() ==0? endIndex -1: endIndex;",
      "for(inti = startIndex; i <= endIndex; i++)",
      "{",
      "longoffset = i *8;",
      "...",
      "}",
      "}",
      "returnoffsets.toArray(newChunk[offsets.size()]);",
      "}",
      "",
      ""
    ]
  }
}