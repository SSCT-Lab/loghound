{
  "p": [
    "HDFS-1371"
  ],
  "One bad node can incorrectly flag many files as corrupt": {},
  "(1) Log information": {
    "(1.1) Roles in this case": {
      "p": [
        "DFSClient (client-side) NameNode (server-side) DataNodes (not directly related)"
      ]
    },
    "(1.2) Symptoms": {
      "p": [
        "Description: On our cluster, 12 files were reported as corrupt by fsck even though the replicas on the datanodes were healthy. Turns out that all the replicas (12 files x 3 replicas per file) were reported corrupt from one node(DFSClient). Surprisingly, these files were still readable/accessible from other DFSClient (-get/-cat) without any problems.",
        "Take one such file as an example: /myfile/part-00145.gz blk_-1426587446408804113_970819282",
        "Namenode log shows",
        "2010-08-31 10:47:56,258 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: blk_-1426587446408804113 added as corrupt onZZ.YY.XX..220:1004 by /ZZ.YY.XX.246",
        "2010-08-31 10:47:56,290 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: blk_-1426587446408804113 added as corrupt onZZ.YY.XX..252:1004 by /ZZ.YY.XX.246",
        "2010-08-31 10:47:56,489 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: blk_-1426587446408804113 added as corrupt onZZ.YY.XX..107:1004 by /ZZ.YY.XX.246",
        "2010-08-31 10:49:00,508 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap:duplicate requestedfor blk_-1426587446408804113 to add as corrupt onZZ.YY.XX.252:1004 by /ZZ.YY.XX.246",
        "2010-08-31 10:49:00,554 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap:duplicate requestedfor blk_-1426587446408804113 to add as corrupt onZZ.YY.XX.107:1004 by /ZZ.YY.XX.246",
        "2010-08-31 10:49:03,934 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap:duplicate requestedfor blk_-1426587446408804113 to add as corrupt onZZ.YY.XX.220:1004 by /ZZ.YY.XX.246",
        "…",
        "DFSClient on ZZ.YY.XX.246 continue to sends request to NameNode, in order to add three block replicas for blk_-1426587446408804113 as corrupted.",
        "User Tasklogs(DFSClient) on ZZ.YY.XX.246 shows",
        "[root@ZZ.YY.XX.246 ~]# find /my/mapred/userlogs/ -type f -exec grep 1426587446408804113 \\{\\} \\; -print",
        "org.apache.hadoop.fs.ChecksumException: Checksum error: /blk_-1426587446408804113:of:/myfile/part-00145.gz at 222720",
        "2010-08-31 10:47:56,256 WARN org.apache.hadoop.hdfs.DFSClient: Found Checksum error for blk_-1426587446408804113_970819282 fromZZ.YY.XX.220:1004at 222720",
        "org.apache.hadoop.fs.ChecksumException: Checksum error: /blk_-1426587446408804113:of:/myfile/part-00145.gz at 103936",
        "2010-08-31 10:47:56,284 WARN org.apache.hadoop.hdfs.DFSClient: Found Checksum error for blk_-1426587446408804113_970819282 fromZZ.YY.XX.252:1004at 103936",
        "org.apache.hadoop.fs.ChecksumException: Checksum error: /blk_-1426587446408804113:of:/myfile/part-00145.gz at 250368",
        "2010-08-31 10:47:56,464 WARN org.apache.hadoop.hdfs.DFSClient: Found Checksum error for blk_-1426587446408804113_970819282 fromZZ.YY.XX.107:1004at 250368",
        "2010-08-31 10:47:56,490 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain block blk_-1426587446408804113_970819282 from any node: java.io.IOException:No live nodes contain current block. Will get new block locations from namenode and retry...",
        "The DFSClient on ZZ.YY.XX.246 reports that all the three replicas for blk_-1426587446408804113 are troubled with checksum error.",
        ""
      ]
    }
  },
  "(2) How to figure out the root cause based on logs": {
    "p": [
      "After checking the related code, we can get the following procedures:",
      "(a) When DFSClient detects a corrupt replica, it reports to NN. Then, NN will blindly mark the replica as corrupted in the BlocksMap.",
      "(b) When NN receives a ClientProtocol.getBlockLocations(..) rpc call, it gets all the replicas from the BlocksMap. If there are one or more good replicas, NN returns the good replicas only. If all replicas are corrupted, it returns all (corrupted) replicas and set LocatedBlock.corrupt = true.",
      "(c) When DFSClient gets a LocatedBlock from NN, it does not care whether LocatedBlock.corrupt is true or false.",
      "The flaws are in (a) and (c):",
      "If the DFSClient in (a) is bad (e.g. bad machine), NN may incorrectly mark the replicas as corrupted. Then, when another DFSClient tries to read the block, it receives a LocatedBlock with LocatedBlock.corrupt = true but it still keeps using them because of (c).",
      "Luckily, the double negative cancels out, therefore, the read successes. However, the NN BlocksMap information is incorrect and will not be fixed until NN restarts."
    ]
  },
  "(3) Root Cause": {
    "p": [
      "A bad DFSClient (e.g., on a bad machine) fails to read all the replicas for the block, and makes NN incorrectly mark the replicas as corrupted. The other \"healthy\" DFSClient can still access/read the block."
    ]
  },
  "(4) Fixing Method": {
    "p": [
      "Several developers have discussed a lot, and they came up with three solutions:",
      "1) If DFSClient has tried multiple data nodes and at least one of the attempts has succeeded, it reports the other failures as corrupted block to NN. If DFSClient has tried out all data nodes but all failed, it does not report anything to NN. Since this maybe due to the DFSClient is a handicapped client (who can not read). DFSClient only reports if the total number of replica is 1.",
      "2) After DFSclient detects a bad block replica, it reports back to that DN directly by sending a OP_STATUS_CHECKSUM_ERROR message. The DN puts these blocks to the head of block scanner to verify. If the replica is bad, repair as how block scanner is now doing. This way no traffic driven to NN. Logic changes are in block scanner and adding communication of echo message betweenDNandDFSClient.",
      "3) DFDClient reports to NN, once NN finds thatALLreplicas are bad, NN asks DNs to verify. One drawback would be a bad client can keep reporting to NN. NN can be overworked.",
      "Finally,solution 1) is chosen, and the change only happens on the DFSClient reading logic. It is a light weighted solution with no need of protocol change.",
      "Based on the choice, it shows that the developer hopes to make as little change as possible, in order to minimize the impact scope (the number of the nodes & the extra burden of the node).",
      "With the patch, if it is in the same scenario, the following IOException will still occur in DFSClient’s log, but DFSClient will not send the wrong message (all the block replicas are corrupted) to NN.",
      "2010-08-31 10:47:56,490 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain block blk_-1426587446408804113_970819282 from any node: java.io.IOException:No live nodes contain current block. Will get new block locations from namenode and retry...",
      "",
      "The reason for choosing solution 1):",
      "1)we consider a bad client is a client who has \"good wish\" but handicapped with some physical difficulties (such as memory problem), not a malicious client.",
      "2) If a client can not even read one good replica, it could be a handicapped client. In the case of all replicas are corrupted, there is nothing cluster can do to recover. Reporting this to the NN would not make a difference. Moreover, based on one developer’s comment, it has never been a case that all the replicas of a block are all corrupted in their production environment in the past years.",
      "3)Handicapped client is extremely rare. they do not want to put in heavy verification logic on the NN or DN and neither want to have protocol change to just verify blocks for this extremely rare case of handicapped client.",
      "",
      "The patch:",
      "Make the oldreportCheckSumFailure() function in DistributedFileSystem.java “deprecated”. And add the newreportCheckSumFailure() function in DFSInputStream.java.",
      "·src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "",
      "The idea of this patch: When failing with the block replica, add the corrupted block replica into map by the new functionaddIntoCorruptedBlockMap().Check if need to report block replicas corruption in “finally” by the new functionreportCheckSumFailure().",
      "",
      "@@ -462,8 +470,9 @@",
      "* name readBuffer() is chosen to imply similarity to readBuffer() in",
      "* ChecksuFileSystem",
      "*/",
      "- private synchronized int readBuffer(byte buf[], int off, int len)",
      "- throws IOException {",
      "+ private synchronized intreadBuffer(byte buf[], int off, int len,",
      "+ Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap)",
      "+ throws IOException {",
      "IOException ioe;",
      "",
      "/* we retry current node only once. So this is set to true only here.",
      "@@ -479,16 +488,19 @@",
      "try {",
      "return blockReader.read(buf, off, len);",
      "} catch ( ChecksumException ce ) {",
      "- DFSClient.LOG.warn(\"Found Checksum error for \" + currentBlock + \" from \" +",
      "- currentNode.getName() + \" at \" + ce.getPos());",
      "- dfsClient.reportChecksumFailure(src, currentBlock, currentNode);",
      "+ DFSClient.LOG.warn(\"Found Checksum error for \"",
      "+ + getCurrentBlock() + \" from \" + currentNode.getName()",
      "+ + \" at \" + ce.getPos());",
      "ioe = ce;",
      "retryCurrentNode = false;",
      "+ // we want to remember which block replicas we have tried",
      "+addIntoCorruptedBlockMap(getCurrentBlock(), currentNode,",
      "+ corruptedBlockMap);",
      "} catch ( IOException e ) {",
      "if (!retryCurrentNode) {",
      "- DFSClient.LOG.warn(\"Exception while reading from \" + currentBlock +",
      "- \" of \" + src + \" from \" + currentNode + \": \" +",
      "- StringUtils.stringifyException(e));",
      "+ DFSClient.LOG.warn(\"Exception while reading from \"",
      "+ + getCurrentBlock() + \" of \" + src + \" from \"",
      "+ + currentNode + \": \" + StringUtils.stringifyException(e));",
      "}",
      "ioe = e;",
      "}",
      "@@ -551,12 +565,34 @@",
      "if (--retries == 0) { throw e; }",
      "+ } finally {",
      "+ // Check if need to report block replicas corruption either read",
      "+ // was successful or ChecksumException occured.",
      "+ reportCheckSumFailure(corruptedBlockMap,",
      "+ currentLocatedBlock.getLocations().length);",
      "}",
      "}",
      "}",
      "return -1;",
      "}",
      "+ /**",
      "+ * Add corrupted block replica into map.",
      "+ * @param corruptedBlockMap",
      "+ */",
      "+ private voidaddIntoCorruptedBlockMap(ExtendedBlock blk, DatanodeInfo node,",
      "+ Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap) {",
      "+ Set<DatanodeInfo> dnSet = null;",
      "+ if((corruptedBlockMap.containsKey(blk))) {",
      "+ dnSet = corruptedBlockMap.get(blk);",
      "+ }else {",
      "+ dnSet = new HashSet<DatanodeInfo>();",
      "+ }",
      "+ if (!dnSet.contains(node)) {",
      "+ dnSet.add(node);",
      "+ corruptedBlockMap.put(blk, dnSet);",
      "+ }",
      "+ }",
      "",
      "private DNAddrPair chooseDataNode(LocatedBlock block)",
      "throws IOException {",
      "@@ -605,8 +641,10 @@",
      "}",
      "}",
      "",
      "- private void fetchBlockByteRange(LocatedBlock block, long start,",
      "- long end, byte[] buf, int offset) throws IOException {",
      "+ private voidfetchBlockByteRange(LocatedBlock block, long start, long end,",
      "+ byte[] buf, int offset,",
      "+ Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap)",
      "+ throws IOException {",
      "//",
      "// Connect to best DataNode for desired Block, with potential offset",
      "//",
      "@@ -646,7 +684,8 @@",
      "DFSClient.LOG.warn(\"fetchBlockByteRange(). Got a checksum exception for \" +",
      "src + \" at \" + block.getBlock() + \":\" +",
      "e.getPos() + \" from \" + chosenNode.getName());",
      "- dfsClient.reportChecksumFailure(src, block.getBlock(), chosenNode);",
      "+ // we want to remember what we have tried",
      "+addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);",
      "} catch (IOException e) {",
      "if (e instanceof InvalidBlockTokenException && refetchToken > 0) {",
      "DFSClient.LOG.info(\"Will get a new access token and retry, \"",
      "@@ -703,11 +742,21 @@in function read(){}:",
      "// corresponding to position and realLen",
      "List<LocatedBlock> blockRange = getBlockRange(position, realLen);",
      "int remaining = realLen;",
      "+ Map<ExtendedBlock,Set<DatanodeInfo>> corruptedBlockMap",
      "+ = new HashMap<ExtendedBlock, Set<DatanodeInfo>>();",
      "for (LocatedBlock blk : blockRange) {",
      "long targetStart = position - blk.getStartOffset();",
      "long bytesToRead = Math.min(remaining, blk.getBlockSize() - targetStart);",
      "- fetchBlockByteRange(blk, targetStart,",
      "- targetStart + bytesToRead - 1, buffer, offset);",
      "+ try {",
      "+ fetchBlockByteRange(blk, targetStart,",
      "+ targetStart + bytesToRead - 1, buffer, offset, corruptedBlockMap);",
      "+ } finally {",
      "+ // Check and report if any block replicas are corrupted.",
      "+ // BlockMissingException may be caught if all block replicas are",
      "+ // corrupted.",
      "+ reportCheckSumFailure(corruptedBlockMap, blk.getLocations().length);",
      "+ }",
      "+",
      "remaining -= bytesToRead;",
      "position += bytesToRead;",
      "offset += bytesToRead;",
      "@@ -718,7 +767,43 @@",
      "}",
      "return realLen;",
      "}",
      "-",
      "+",
      "+ /**",
      "+ * DFSInputStream reports checksum failure.",
      "+ * Case I : client has tried multiple data nodes and at least one of the",
      "+ * attempts has succeeded. We report the other failures as corrupted block to",
      "+ * namenode.",
      "+ * Case II: client has tried out all data nodes, but all failed. We",
      "+ * only report if the total number of replica is 1. We do not",
      "+ * report otherwise since this maybe due to the client is a handicapped client",
      "+ * (who can not read).",
      "+ * @param corruptedBlockMap, map of corrupted blocks",
      "+ * @param dataNodeCount, number of data nodes who contains the block replicas",
      "+ */",
      "+ private voidreportCheckSumFailure(",
      "+ Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap,",
      "+ int dataNodeCount) {",
      "+ if (corruptedBlockMap.isEmpty()) {",
      "+ return;",
      "+ }",
      "+ Iterator<Entry<ExtendedBlock, Set<DatanodeInfo>>> it = corruptedBlockMap",
      "+ .entrySet().iterator();",
      "+ Entry<ExtendedBlock, Set<DatanodeInfo>> entry = it.next();",
      "+ ExtendedBlock blk = entry.getKey();",
      "+ Set<DatanodeInfo> dnSet = entry.getValue();",
      "+ if (((dnSet.size() < dataNodeCount) && (dnSet.size() > 0))",
      "+ || ((dataNodeCount == 1) && (dnSet.size() == dataNodeCount))) {",
      "+ DatanodeInfo[] locs = new DatanodeInfo[dnSet.size()];",
      "+ int i = 0;",
      "+ for (DatanodeInfo dn:dnSet) {",
      "+ locs[i++] = dn;",
      "+ }",
      "+ LocatedBlock [] lblocks = { new LocatedBlock(blk, locs) };",
      "+ dfsClient.reportChecksumFailure(src, lblocks);",
      "+ }",
      "+ corruptedBlockMap.clear();",
      "+ }",
      "+",
      "@Override",
      "public long skip(long n) throws IOException {",
      "if ( n > 0 ) {",
      "@@ -760,9 +845,10 @@",
      "}",
      "} catch (IOException e) {//make following read to retry",
      "if(DFSClient.LOG.isDebugEnabled()) {",
      "- DFSClient.LOG.debug(\"Exception while seek to \" + targetPos +",
      "- \" from \" + currentBlock +\" of \" + src + \" from \" +",
      "- currentNode + \": \" + StringUtils.stringifyException(e));",
      "+ DFSClient.LOG.debug(\"Exception while seek to \" + targetPos",
      "+ + \" from \" + getCurrentBlock() + \" of \" + src",
      "+ + \" from \" + currentNode + \": \"",
      "+ + StringUtils.stringifyException(e));",
      "}",
      "}",
      "}",
      "",
      "· src/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
      "@@ -689,6 +689,9 @@",
      "* is corrupt but we will report both to the namenode. In the future,",
      "* we can consider figuring out exactly which block is corrupt.",
      "*/",
      "+ // We do not see a need for user to report block checksum errors and do not",
      "+ // want to rely on user to report block corruptions.",
      "+ @Deprecated",
      "public boolean reportChecksumFailure(Path f,",
      "FSDataInputStream in, long inPos,",
      "FSDataInputStream sums, long sumsPos) {",
      "",
      "",
      "The other fixes in file DFSInputStream.java",
      "@@ -62,7 +67,7 @@",
      "private LocatedBlocks locatedBlocks = null;",
      "private long lastBlockBeingWrittenLength = 0;",
      "private DatanodeInfo currentNode = null;",
      "- private ExtendedBlock currentBlock = null;",
      "+ private LocatedBlock currentLocatedBlock = null;",
      "private long pos = 0;",
      "private long blockEnd = -1;",
      "@@ -204,8 +209,11 @@",
      "/**",
      "* Returns the block containing the target position.",
      "*/",
      "- public ExtendedBlock getCurrentBlock() {",
      "- return currentBlock;",
      "+ synchronized public ExtendedBlock getCurrentBlock() {",
      "+ if (currentLocatedBlock == null){",
      "+ return null;",
      "+ }",
      "+ return currentLocatedBlock.getBlock();",
      "}",
      "/**",
      "@@ -261,7 +269,7 @@",
      "if (updatePosition) {",
      "pos = offset;",
      "blockEnd = blk.getStartOffset() + blk.getBlockSize() - 1;",
      "- currentBlock = blk.getBlock();",
      "+ currentLocatedBlock = blk;",
      "}",
      "return blk;",
      "}",
      "",
      "@@ -519,6 +531,8 @@",
      "if (closed) {",
      "throw new IOException(\"Stream closed\");",
      "}",
      "+ Map<ExtendedBlock,Set<DatanodeInfo>> corruptedBlockMap",
      "+ = new HashMap<ExtendedBlock, Set<DatanodeInfo>>();",
      "failures = 0;",
      "if (pos < getFileLength()) {",
      "int retries = 2;",
      "@@ -528,7 +542,7 @@",
      "currentNode = blockSeekTo(pos);",
      "}",
      "int realLen = (int) Math.min((long) len, (blockEnd - pos + 1L));",
      "- int result = readBuffer(buf, off, realLen);",
      "+ int result = readBuffer(buf, off, realLen, corruptedBlockMap);",
      "",
      "if (result >= 0) {",
      "pos += result;",
      ""
    ]
  },
  "(5) How many nodes are involved in the patch": {
    "p": [
      "The patch is in DFSClient (one or multiple nodes)",
      ""
    ]
  }
}